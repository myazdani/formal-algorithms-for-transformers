# Formal Algorithms For Transformers

PyTorch implementation of transformer algorithms described in "Formal Algorithms for Transformers" by Mary Phuong and Marcus Hutter:  https://arxiv.org/abs/2207.09238

[Algorithm 1](https://github.com/myazdani/formal-algorithms-for-transformers/blob/main/src/alg_1.py): token embedding
Algorithm 2: positional embedding
Algorithm 3: Basic single-query attention
Algorithm 4: ğ‘½Ëœ â† Attention(ğ‘¿, ğ’|Wğ’’ğ’Œğ’—, Mask)
Algorithm 5: ğ‘½Ëœ â† MHAttention(ğ‘¿, ğ’|W, Mask)
Algorithm 6: Ë†ğ’† â† layer_norm(ğ’†|ğœ¸, ğœ·)
Algorithm 7: Unembedding.
Algorithm 8: ğ‘· â† EDTransformer(ğ’›, ğ’™|ğœ½)
Algorithm 9: ğ‘· â† ETransformer(ğ’™|ï¿½
Algorithm 10: ğ‘· â† DTransformer(ğ’™|ğœ½)
Algorithm 11: ğœ½Ë† â† EDTraining(ğ’›1:ğ‘data , ğ’™1:ğ‘data , ğœ½)
Algorithm 12: ğœ½Ë† â† ETraining(ğ’™1:ğ‘data , ğœ½)
Algorithm 13: ğœ½Ë† â† DTraining(ğ’™1:ğ‘data , ğœ½)
Algorithm 14: ğ’š â† DInference(ğ’™, ğœ½Ë†)
