# Formal Algorithms For Transformers

PyTorch implementation of transformer algorithms described in "Formal Algorithms for Transformers" by Mary Phuong and Marcus Hutter:  https://arxiv.org/abs/2207.09238

[Algorithm 1](https://github.com/myazdani/formal-algorithms-for-transformers/blob/main/src/alg_1.py): token embedding

[Algorithm 2](https://github.com/myazdani/formal-algorithms-for-transformers/blob/main/src/alg_2.py): positional embedding

[Algorithm 3](https://github.com/myazdani/formal-algorithms-for-transformers/blob/main/src/alg_3.py): Basic single-query attention

[Algorithm 4](https://github.com/myazdani/formal-algorithms-for-transformers/blob/main/src/alg_4.py): ğ‘½Ëœ â† Attention(ğ‘¿, ğ’|Wğ’’ğ’Œğ’—, Mask)

[Algorithm 5](https://github.com/myazdani/formal-algorithms-for-transformers/blob/main/src/alg_5.py): ğ‘½Ëœ â† MHAttention(ğ‘¿, ğ’|W, Mask)

[Algorithm 6](https://github.com/myazdani/formal-algorithms-for-transformers/blob/main/src/alg_6.py): Ë†ğ’† â† layer_norm(ğ’†|ğœ¸, ğœ·)

[Algorithm 7](https://github.com/myazdani/formal-algorithms-for-transformers/blob/main/src/alg_7.py): Unembedding.

[Algorithm 8](https://github.com/myazdani/formal-algorithms-for-transformers/blob/main/src/alg_8.py): ğ‘· â† EDTransformer(ğ’›, ğ’™|ğœ½)

[Algorithm 9](https://github.com/myazdani/formal-algorithms-for-transformers/blob/main/src/alg_9.py): ğ‘· â† ETransformer(ğ’™|ğœ½)

[Algorithm 10](https://github.com/myazdani/formal-algorithms-for-transformers/blob/main/src/alg_10.py): ğ‘· â† DTransformer(ğ’™|ğœ½)

Algorithm 11: ğœ½Ë† â† EDTraining(ğ’›1:ğ‘data , ğ’™1:ğ‘data , ğœ½)

Algorithm 12: ğœ½Ë† â† ETraining(ğ’™1:ğ‘data , ğœ½)

Algorithm 13: ğœ½Ë† â† DTraining(ğ’™1:ğ‘data , ğœ½)

Algorithm 14: ğ’š â† DInference(ğ’™, ğœ½Ë†)
